{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-13T09:44:24.824429Z","iopub.execute_input":"2022-01-13T09:44:24.824861Z","iopub.status.idle":"2022-01-13T09:44:24.860191Z","shell.execute_reply.started":"2022-01-13T09:44:24.824745Z","shell.execute_reply":"2022-01-13T09:44:24.859016Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dropout, Dense, LSTM\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport math\n\nmaotai = pd.read_csv('../input/stocks-data/SH600519.csv')  # 读取股票文件","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:46:57.613974Z","iopub.execute_input":"2022-01-13T09:46:57.614228Z","iopub.status.idle":"2022-01-13T09:46:57.636382Z","shell.execute_reply.started":"2022-01-13T09:46:57.614199Z","shell.execute_reply":"2022-01-13T09:46:57.635689Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**数据集的制作**","metadata":{}},{"cell_type":"code","source":"\ntraining_set = maotai.iloc[0:2426 - 300, 2:3].values  # 前(2426-300=2126)天的开盘价作为训练集,表格从0开始计数，2:3 是提取[2:3)列，前闭后开,故提取出C列开盘价\ntest_set = maotai.iloc[2426 - 300:, 2:3].values  # 后300天的开盘价作为测试集\n\n# 归一化\nsc = MinMaxScaler(feature_range=(0, 1))  # 定义归一化：归一化到(0，1)之间\ntraining_set_scaled = sc.fit_transform(training_set)  # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化\ntest_set = sc.transform(test_set)  # 利用训练集的属性对测试集进行归一化\n\nx_train = []\ny_train = []\n\nx_test = []\ny_test = []\n\n# 测试集：csv表格中前2426-300=2126天数据\n# 利用for循环，遍历整个训练集，提取训练集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建2426-300-60=2066组数据。\nfor i in range(60, len(training_set_scaled)):\n    x_train.append(training_set_scaled[i - 60:i, 0])\n    y_train.append(training_set_scaled[i, 0])\n# 对训练集进行打乱\nnp.random.seed(7)\nnp.random.shuffle(x_train)\nnp.random.seed(7)\nnp.random.shuffle(y_train)\ntf.random.set_seed(7)\n# 将训练集由list格式变为array格式\nx_train, y_train = np.array(x_train), np.array(y_train)\n\n# 使x_train符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。\n# 此处整个数据集送入，送入样本数为x_train.shape[0]即2066组数据；输入60个开盘价，预测出第61天的开盘价，循环核时间展开步数为60; 每个时间步送入的特征是某一天的开盘价，只有1个数据，故每个时间步输入特征个数为1\nx_train = np.reshape(x_train, (x_train.shape[0], 60, 1))\n# 测试集：csv表格中后300天数据\n# 利用for循环，遍历整个测试集，提取测试集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建300-60=240组数据。\nfor i in range(60, len(test_set)):\n    x_test.append(test_set[i - 60:i, 0])\n    y_test.append(test_set[i, 0])\n# 测试集变array并reshape为符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]\nx_test, y_test = np.array(x_test), np.array(y_test)\nx_test = np.reshape(x_test, (x_test.shape[0], 60, 1))","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:47:01.051399Z","iopub.execute_input":"2022-01-13T09:47:01.051670Z","iopub.status.idle":"2022-01-13T09:47:01.076051Z","shell.execute_reply.started":"2022-01-13T09:47:01.051641Z","shell.execute_reply":"2022-01-13T09:47:01.075316Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**模型构建**","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    LSTM(80, return_sequences=True),\n    Dropout(0.2),\n    LSTM(100),\n    Dropout(0.2),\n    Dense(1)\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n              loss='mean_squared_error')  # 损失函数用均方误差\n# 该应用只观测loss数值，不观测准确率，所以删去metrics选项，一会在每个epoch迭代显示时只显示loss值\n\ncheckpoint_save_path = \"./checkpoint/LSTM_stock.ckpt\"\n\nif os.path.exists(checkpoint_save_path + '.index'):\n    print('-------------load the model-----------------')\n    model.load_weights(checkpoint_save_path)\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,\n                                                 save_weights_only=True,\n                                                 save_best_only=True,\n                                                 monitor='val_loss')\n\nhistory = model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), validation_freq=1,\n                    callbacks=[cp_callback])\n\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:47:04.964117Z","iopub.execute_input":"2022-01-13T09:47:04.964726Z","iopub.status.idle":"2022-01-13T09:47:27.852707Z","shell.execute_reply.started":"2022-01-13T09:47:04.964686Z","shell.execute_reply":"2022-01-13T09:47:27.852017Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**参数提取 训练效果可视化**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\nfile = open('./weights.txt', 'w')  # 参数提取\nfor v in model.trainable_variables:\n    file.write(str(v.name) + '\\n')\n    file.write(str(v.shape) + '\\n')\n    file.write(str(v.numpy()) + '\\n')\nfile.close()\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:47:33.484330Z","iopub.execute_input":"2022-01-13T09:47:33.484927Z","iopub.status.idle":"2022-01-13T09:47:33.731881Z","shell.execute_reply.started":"2022-01-13T09:47:33.484887Z","shell.execute_reply":"2022-01-13T09:47:33.731196Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**预测效果**","metadata":{}},{"cell_type":"code","source":" \npredicted_stock_price = model.predict(x_test)\n# 对预测数据还原---从（0，1）反归一化到原始范围\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)\n# 对真实数据还原---从（0，1）反归一化到原始范围\nreal_stock_price = sc.inverse_transform(test_set[60:])\n# 画出真实数据和预测数据的对比曲线\nplt.plot(real_stock_price, color='red', label='MaoTai Stock Price')\nplt.plot(predicted_stock_price, color='blue', label='Predicted MaoTai Stock Price')\nplt.title('MaoTai Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('MaoTai Stock Price')\nplt.legend()\nplt.show()\n\n##########evaluate##############\n# calculate MSE 均方误差 ---> E[(预测值-真实值)^2] (预测值减真实值求平方后求均值)\nmse = mean_squared_error(predicted_stock_price, real_stock_price)\n# calculate RMSE 均方根误差--->sqrt[MSE]    (对均方误差开方)\nrmse = math.sqrt(mean_squared_error(predicted_stock_price, real_stock_price))\n# calculate MAE 平均绝对误差----->E[|预测值-真实值|](预测值减真实值求绝对值后求均值）\nmae = mean_absolute_error(predicted_stock_price, real_stock_price)\nprint('均方误差: %.6f' % mse)\nprint('均方根误差: %.6f' % rmse)\nprint('平均绝对误差: %.6f' % mae)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T09:47:52.212826Z","iopub.execute_input":"2022-01-13T09:47:52.213163Z","iopub.status.idle":"2022-01-13T09:47:53.012807Z","shell.execute_reply.started":"2022-01-13T09:47:52.213111Z","shell.execute_reply":"2022-01-13T09:47:53.011723Z"},"trusted":true},"execution_count":10,"outputs":[]}]}